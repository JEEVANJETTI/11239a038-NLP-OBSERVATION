{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvBDofVtKkXxRK+sWuqZ8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JEEVANJETTI/11239a038-NLP-OBSERVATION/blob/main/NLP_OBSERVATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlKvAcv8GW1V",
        "outputId": "92bc8b18-f999-4250-9872-9d5026eadaee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Natural', 'Language', 'Processing', 'makes', 'computers', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer models (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "\n",
        "text = \"Natural Language Processing makes computers understand human language.\"\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"Word Tokens:\", words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "text = \"The cars are running faster than the buses.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize every word\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlUQyF_3G9mF",
        "outputId": "68f8c643-7255-4395-8a48-45692a76c527"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'cars', 'are', 'running', 'faster', 'than', 'the', 'buses', '.']\n",
            "Lemmatized Words: ['The', 'car', 'are', 'running', 'faster', 'than', 'the', 'bus', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"The cars are running faster than the buses.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeqvkFExHs5b",
        "outputId": "9f6e2304-e36b-45bc-fc6d-018edf5ad3b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['The', 'cars', 'are', 'running', 'faster', 'than', 'the', 'buses', '.']\n",
            "Stemmed Words: ['the', 'car', 'are', 'run', 'faster', 'than', 'the', 'buse', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"The boys are playing in the gardens.\"\n",
        "\n",
        "# Tokenize\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize tools\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Morphological Analysis:\\n\")\n",
        "\n",
        "for word in words:\n",
        "    print(f\"Word: {word}\")\n",
        "    print(f\"Stem: {stemmer.stem(word)}\")\n",
        "    print(f\"Lemma: {lemmatizer.lemmatize(word)}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QONEFOzeHx57",
        "outputId": "d38a8fb4-c1c6-4a5c-96fe-6b0b3898599d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morphological Analysis:\n",
            "\n",
            "Word: The\n",
            "Stem: the\n",
            "Lemma: The\n",
            "\n",
            "Word: boys\n",
            "Stem: boy\n",
            "Lemma: boy\n",
            "\n",
            "Word: are\n",
            "Stem: are\n",
            "Lemma: are\n",
            "\n",
            "Word: playing\n",
            "Stem: play\n",
            "Lemma: playing\n",
            "\n",
            "Word: in\n",
            "Stem: in\n",
            "Lemma: in\n",
            "\n",
            "Word: the\n",
            "Stem: the\n",
            "Lemma: the\n",
            "\n",
            "Word: gardens\n",
            "Stem: garden\n",
            "Lemma: garden\n",
            "\n",
            "Word: .\n",
            "Stem: .\n",
            "Lemma: .\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "\n",
        "# Initialize spell corrector\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "text = \"I havv a speling misstake in thiss sentense.\"\n",
        "\n",
        "# Correct each word\n",
        "corrected_text = \" \".join([spell(word) for word in text.split()])\n",
        "\n",
        "print(\"Original:\", text)\n",
        "print(\"Corrected:\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOhJkcPOIE1E",
        "outputId": "d8a1b047-6750-4906-f83c-b635ea4dcd89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=99068323680289d24ebcdeb0fdab0dbbad5619b27b64c71d6d0f3cc50219bb5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/28/c2/9ddf8f57f871b55b6fd0ab99c887531fb9a66e5ff236b82aee\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n",
            "Original: I havv a speling misstake in thiss sentense.\n",
            "Corrected: I have a spelling mistake in this sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sem import Expression\n",
        "from nltk.inference import ResolutionProver\n",
        "\n",
        "# Helper to read logical expressions\n",
        "read_expr = Expression.fromstring\n",
        "\n",
        "# Facts (premises)\n",
        "premise1 = read_expr('man(socrates)')\n",
        "premise2 = read_expr('all x. (man(x) -> mortal(x))')\n",
        "\n",
        "# Goal (conclusion)\n",
        "goal = read_expr('mortal(socrates)')\n",
        "\n",
        "# Try to prove the deduction\n",
        "result = ResolutionProver().prove(goal, [premise1, premise2])\n",
        "\n",
        "print(\"Is the conclusion valid?:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OntBjyVrIYv6",
        "outputId": "b1a481f7-48c1-47ad-be82-0845181d5c3f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the conclusion valid?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Natural language processing is interesting\"\n",
        "\n",
        "# Tokenize\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Bigrams (2-grams)\n",
        "bigrams = list(ngrams(words, 2))\n",
        "\n",
        "# Trigrams (3-grams)\n",
        "trigrams = list(ngrams(words, 3))\n",
        "\n",
        "print(\"Words:\", words)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPO7wWI1I9Gl",
        "outputId": "75f8e3fe-679f-4236-afc0-ba3621b54f99"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Natural', 'language', 'processing', 'is', 'interesting']\n",
            "Bigrams: [('Natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'interesting')]\n",
            "Trigrams: [('Natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'interesting')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"natural language processing is interesting and language is powerful\"\n",
        "\n",
        "# Tokenize\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# N-gram size\n",
        "n = 2   # bigram model\n",
        "\n",
        "# Prepare data for language model\n",
        "train_data, vocab = padded_everygram_pipeline(n, [tokens])\n",
        "\n",
        "# Train Laplace smoothed model\n",
        "model = Laplace(n)\n",
        "model.fit(train_data, vocab)\n",
        "\n",
        "# Test probability\n",
        "print(\"P('language' | 'natural') =\", model.score(\"language\", [\"natural\"]))\n",
        "print(\"P('processing' | 'language') =\", model.score(\"processing\", [\"language\"]))\n",
        "print(\"P('xyz' | 'language') =\", model.score(\"xyz\", [\"language\"]))  # unseen word\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQzgJ9XTLWKT",
        "outputId": "69a49be1-8687-4eb3-fea5-d97a584bac56"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P('language' | 'natural') = 0.18181818181818182\n",
            "P('processing' | 'language') = 0.16666666666666666\n",
            "P('xyz' | 'language') = 0.08333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required models (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"Natural language processing makes computers understand human language.\"\n",
        "\n",
        "# Tokenize\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR1Kl55vLdG0",
        "outputId": "ded48d41-5f1d-4f94-e000-5a524caa97c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('makes', 'VBZ'), ('computers', 'NNS'), ('understand', 'JJ'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import brill, brill_trainer\n",
        "\n",
        "# Download required data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"treebank\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "# Load training data\n",
        "train_sents = nltk.corpus.treebank.tagged_sents()[:200]\n",
        "\n",
        "# Base tagger (uses default rules)\n",
        "base_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "# Brill rule templates\n",
        "templates = brill.fntbl37()\n",
        "\n",
        "# Train Brill tagger\n",
        "trainer = brill_trainer.BrillTaggerTrainer(base_tagger, templates)\n",
        "brill_tagger = trainer.train(train_sents)\n",
        "\n",
        "# Test sentence\n",
        "sentence = \"Natural language processing improves machine intelligence\".split()\n",
        "\n",
        "# Brill POS tagging\n",
        "print(brill_tagger.tag(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlhcDroFLlDs",
        "outputId": "2808e549-48a1-4217-e017-332ed527ea00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'NN'), ('language', 'NN'), ('processing', 'NN'), ('improves', 'NN'), ('machine', 'NN'), ('intelligence', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
        "\n",
        "# Download data (run once)\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load tagged sentences\n",
        "tagged_sents = treebank.tagged_sents()[:2000]   # training data\n",
        "train_data = tagged_sents[:1500]\n",
        "test_data = tagged_sents[1500:1600]\n",
        "\n",
        "# Train HMM POS tagger\n",
        "trainer = HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train_supervised(train_data)\n",
        "\n",
        "# Test sentence\n",
        "sentence = nltk.word_tokenize(\"Natural language processing is fun\")\n",
        "print(hmm_tagger.tag(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxrqvRgFLxRX",
        "outputId": "4b435eb2-02c0-4ab2-cdfb-76e077f6fb1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'NNP'), ('language', 'NNP'), ('processing', 'NNP'), ('is', 'NNP'), ('fun', 'NNP')]\n"
          ]
        }
      ]
    }
  ]
}